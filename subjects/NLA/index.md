---
title: Natural Language Applications 
author: Zubair Abid
layout: page
---


## Logistics

**Professor**: Manish Shrivastava

### Grading Scheme

| Component          | Weightage |
|--------------------|-----------|
| Midsem             | 15%       |
| Endsem viva        | 23%       |
| 2 Assignments      | 10%       |
| Reading Assignment | 2%        |
| Project            | 50%       |

## Revision

- [X] [Better Word Based Model slides](./Slides/04-word-based-models.pdf)
- [X] [Better Phrased Based Model slides](./Slides/05-phrase-based-models.pdf)
- [X] [The same decoding slides](./Slides/06-decoding.pdf)
- [X] [Important Alignment Paper (Seq2Seq with Attention)](./Reading/1409.0473.pdf)
- [X] Improvements to Attention (just see abstracts):
    - [X] [Global Coverage](./Reading/1601.04811.pdf)
    - [X] [Agreement](./Reading/1512.04650.pdf)

## Lecture Slides and general readings

- [X] General Reading:
    - [X] [Understanding LSTMS](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- (Statistical) Machine Translation:
    - [Word Based Models](./Slides/Word Based Model.pdf)
    - [Expectation Maximisation](./Slides/Expectation Maximization.pdf)
    - [Word Alignment (Chapter)](./Reading/Word Alignment Full Chapter PDF.pdf))
    - [SMT and Word Alignment - Tutorial Workbook by Kevin Knight](./Reading/A statistical MT tutorial workbook.pdf)
    - [Phrase based, decoding](./Slides/06-decoding.pdf)
- Neural Machine Translation:
    - [Word Embeddings and Word2Vec (Tutorial)](./Reading/Tutorial_1__Introduction_to_Word_Representation.pdf)
    - [Sequence to Sequence (Paper)](./Reading/5346-sequence-to-sequence-learning-with-neural-networks.pdf)
    - [NMT by Jointly Learning to Align and Translate (Paper)](./Reading/1409.0473.pdf)
    - [Word2Vec Paper](./Reading/1301.3781_w2v.pdf)
    - [Seq2seq](./Slides/IASNLP2018-NMT-Part1.pdf)
    - [X] [Attention](./Slides/IASNLP2018-NMT-Part2.pdf):
        - [X] [Luong Attention](./Reading/1508.04025.pdf) (Not in resources, mentioned in class)
        - [X] Coverage
- [X] Transformers:
    - [X] [Paper](./Reading/Attention_Is_All_You_Need.pdf) [notes](notes#attention-is-all-you-need)
    - [X] [transformers: Interactive web page](http://jalammar.github.io/illustrated-transformer/)
    - [X] [Video explanation](https://www.youtube.com/watch?v=iDulhoQ2pro)
- [X] Question Answering:
    - [X] [Slides](./Slides/QA-Overview-1.pdf)
- [X] Papers to read:
    - [X] [Batch Normalisation](./Reading/1502.03167.pdf)
    - [X] [Learning Question Classifiers](./Reading/C02-1150.pdf)
    - [X] [Data-Intensive Question Answering](./Reading/Trec2001Notebook.Feb2002Final.pdf)
    - [X] [Web Question Answering: Is More Always Better?](./Reading/564376.564428.pdf)
    - [X] [Attention Over Attention](./Reading/1607.04423.pdf)
    - [X] [Dynamic Co-Attention Networks (DCN) for QnA](./Reading/1611.01604.pdf)
    - [X] [Bidirectional Attention Flow (BiDAf) for Machine Comprehension](./Reading/1611.01603.pdf)
    - [X] [Reading Wikipedia to Answer Open-Domain Questions (DrQA)](./Reading/1704.00051.pdf)
    - [X] [RankQA](./Reading/RankQA_1906.03008.pdf)




