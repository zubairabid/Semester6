---
title: Natural Language Applications 
author: Zubair Abid
layout: page
---


## Logistics

**Professor**: Manish Shrivastava

### Grading Scheme

| Component | Weightage |
|-----------|-----------|
|           | %         |
|           | %         |
|           | %         |

## Revision

- [ ] [Better Word Based Model slides](./Slides/04-word-based-models.pdf)
- [ ] [Better Phrased Based Model slides](./Slides/05-phrase-based-models.pdf)
- [ ] [The same decoding slides](./Slides/06-decoding.pdf)
- [ ] [Important Alignment Paper (Seq2Seq with Attention)](./Reading/1409.0473.pdf)
- [ ] Improvements to Attention (just see abstracts):
    - [ ] [Global Coverage](./Reading/1601.04811.pdf)
    - [ ] [Agreement](./Reading/1512.04650.pdf)

## Lecture Slides and general readings

- [X] General Reading:
    - [X] [Understanding LSTMS](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- (Statistical) Machine Translation:
    - [Word Based Models](./Slides/Word Based Model.pdf)
    - [Expectation Maximisation](./Slides/Expectation Maximization.pdf)
    - [Word Alignment (Chapter)](./Reading/Word Alignment Full Chapter PDF.pdf))
    - [SMT and Word Alignment - Tutorial Workbook by Kevin Knight](./Reading/A statistical MT tutorial workbook.pdf)
    - [Phrase based, decoding](./Slides/06-decoding.pdf)
- Neural Machine Translation:
    - [Word Embeddings and Word2Vec (Tutorial)](./Reading/Tutorial_1__Introduction_to_Word_Representation.pdf)
    - [Sequence to Sequence (Paper)](./Reading/5346-sequence-to-sequence-learning-with-neural-networks.pdf)
    - [NMT by Jointly Learning to Align and Translate (Paper)](./Reading/1409.0473.pdf)
    - [Word2Vec Paper](./Reading/1301.3781_w2v.pdf)
    - [Seq2seq](./Slides/IASNLP2018-NMT-Part1.pdf)
    - [ ] [Attention](./Slides/IASNLP2018-NMT-Part2.pdf):
        - [ ] [Luong Attention](./Reading/1508.04025.pdf) (Not in resources, mentioned in class)
        - [ ] Coverage
- [ ] Transformers:
    - [ ] [Paper](./Reading/Attention_Is_All_You_Need.pdf) [notes](notes#attention-is-all-you-need)
    - [ ] [transformers: Interactive web page](http://jalammar.github.io/illustrated-transformer/)
    - [ ] [Video explanation](https://www.youtube.com/watch?v=iDulhoQ2pro)
- [ ] Question Answering:
    - [ ] [Slides](./Slides/QA-Overview-1.pdf)
- [ ] Papers to read:
    - [ ] [Batch Normalisation](./Reading/1502.03167.pdf)
    - [ ] [Learning Question Classifiers](./Reading/C02-1150.pdf)
    - [ ] [Data-Intensive Question Answering](./Reading/Trec2001Notebook.Feb2002Final.pdf)
    - [ ] [Web Question Answering: Is More Always Better?](./Reading/564376.564428.pdf)
    - [ ] [Dynamic Co-Attention Networks (DCN) for QnA](./Reading/1611.01604.pdf)
    - [ ] [Bidirectional Attention Flow (BiDAf) for Machine Comprehension](./Reading/1611.01603.pdf)
    - [ ] [Reading Wikipedia to Answer Open-Domain Questions (DrQA)](./Reading/1704.00051.pdf)
    - [ ] [RankQA](./Reading/RankQA_1906.03008.pdf)




