---
title: NLA Notes
author: Zubair Abid
---

# Attention Is All You Need

## Overview

- Sequential nature of RNN-based models introduces memory constraints and 
  issues with batching
- Removes recurrence, uses only attention mechanism for global dependencies
- Uses self-attention

## References to check

- Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks.arXiv preprintarXiv:1703.10722, 2017.
- Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,and Jeff Dean.  Outrageously large neural networks:  The sparsely-gated mixture-of-expertslayer.arXiv preprint arXiv:1701.06538, 2017.
