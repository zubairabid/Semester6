{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdirs = ['Training','Development','Testing']\n",
    "dir_prefix = os.path.join(os.getcwd(), 'AnnCorra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 28/932 [00:00<00:03, 274.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing files in directory:  /home/zubair/Documents/Work/Acads/Semester6/subjects/LData/Assignment/4/AnnCorra/Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 140/932 [00:00<00:03, 206.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with sentence in file: /home/zubair/Documents/Work/Acads/Semester6/subjects/LData/Assignment/4/AnnCorra/Training/fullnews_id_2506477_date_7_6_2004.dat , no need to interfere\n",
      "Error with sentence in file: /home/zubair/Documents/Work/Acads/Semester6/subjects/LData/Assignment/4/AnnCorra/Training/fullnews_id_2506477_date_7_6_2004.dat , no need to interfere\n",
      "Error with sentence in file: /home/zubair/Documents/Work/Acads/Semester6/subjects/LData/Assignment/4/AnnCorra/Training/fullnews_id_2529450_date_16_6_2004.dat , no need to interfere\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 248/932 [00:01<00:02, 249.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with sentence in file: /home/zubair/Documents/Work/Acads/Semester6/subjects/LData/Assignment/4/AnnCorra/Training/fullnews_id_2564256_date_1_7_2004.dat , no need to interfere\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 932/932 [00:04<00:00, 231.40it/s]\n",
      " 12%|█▏        | 13/112 [00:00<00:00, 126.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing files in directory:  /home/zubair/Documents/Work/Acads/Semester6/subjects/LData/Assignment/4/AnnCorra/Development\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [00:00<00:00, 204.87it/s]\n",
      " 16%|█▌        | 21/131 [00:00<00:00, 205.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing files in directory:  /home/zubair/Documents/Work/Acads/Semester6/subjects/LData/Assignment/4/AnnCorra/Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:00<00:00, 219.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testfile written\n"
     ]
    }
   ],
   "source": [
    "def get_features(pos, featurelist):\n",
    "    featuredict = {}\n",
    "    featuredict['pos'] = pos\n",
    "    featuresplit = featurelist.split('|')\n",
    "    for feature in featuresplit:\n",
    "        key = feature.split('-')[0]\n",
    "        val = feature.split('-')[1]\n",
    "        featuredict[key] = val\n",
    "    return featuredict\n",
    "    \n",
    "for current in subdirs:\n",
    "    all_sentences = []\n",
    "    all_features = []\n",
    "    all_ancorra_tags = []\n",
    "    \n",
    "    directory = os.path.join(dir_prefix, current)\n",
    "    print(\"processing files in directory: \", directory)\n",
    "    for file in tqdm(os.listdir(directory)):\n",
    "        file = os.path.join(directory, file)\n",
    "        with open(file, newline='') as csvfile:\n",
    "            csvreader = csv.reader(csvfile, delimiter='\\t')\n",
    "            \n",
    "            sentence = []\n",
    "            ancorra_tags = []\n",
    "            features = []\n",
    "            \n",
    "            for row in csvreader:\n",
    "                if len(row) == 0:\n",
    "                    all_sentences.append(sentence)\n",
    "                    all_features.append(features)\n",
    "                    all_ancorra_tags.append(ancorra_tags)\n",
    "                    \n",
    "                    sentence = []\n",
    "                    features = []\n",
    "                    ancorra_tags = []\n",
    "                else:\n",
    "                    try:\n",
    "                        sentence.append(row[1])\n",
    "                        features.append(get_features(row[4], row[5]))\n",
    "                        ancorra_tags.append(row[7])\n",
    "                    except:\n",
    "                        print(\"Error with sentence in file:\", file, \", no need to interfere\")\n",
    "    datadir = os.path.join(os.getcwd(), 'Data/')                    \n",
    "    with open(datadir+current+'_sentences.pkl', 'wb') as dump:\n",
    "        pickle.dump(all_sentences, dump)\n",
    "    with open(datadir+current+'_features.pkl', 'wb') as dump:\n",
    "        pickle.dump(all_features, dump)\n",
    "    with open(datadir+current+'_ancorra_tags.pkl', 'wb') as dump:\n",
    "        pickle.dump(all_ancorra_tags, dump)\n",
    "        \n",
    "    if current == 'Testing':\n",
    "        testcases = {}\n",
    "        testcases['sentences'] = all_sentences\n",
    "        testcases['ancorra'] = all_ancorra_tags\n",
    "        testcases['features'] = all_features\n",
    "        with open('testfile.pkl', 'wb') as testfile:\n",
    "            pickle.dump(testcases, testfile)\n",
    "        print('testfile written')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('AnnCorra/Training/fullnews_id_2575975_date_7_7_2004.dat', newline='') as csvfile:\n",
    "#     csvreader = csv.reader(csvfile, delimiter='\\t')\n",
    "#     for row in csvreader:\n",
    "#         if len(row) > 0:\n",
    "#             print(row[1], row[4], row[5].split('|'), row[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804\n",
      "[{'pos': 'PRP', 'cat': 'pn', 'gen': 'any', 'num': 'sg', 'pers': '3h', 'case': 'o', 'vib': 'ने', 'tam': 'ne', 'chunkId': 'NP', 'chunkType': 'head', 'stype': '', 'voicetype': ''}, {'pos': 'VM', 'cat': 'v', 'gen': 'm', 'num': 'sg', 'pers': 'any', 'case': '', 'vib': 'या', 'tam': 'yA', 'chunkId': 'VGF', 'chunkType': 'head', 'stype': 'declarative', 'voicetype': 'active'}, {'pos': 'CC', 'cat': 'avy', 'gen': '', 'num': '', 'pers': '', 'case': '', 'vib': '', 'tam': '', 'chunkId': 'CCP', 'chunkType': 'head', 'stype': '', 'voicetype': ''}, {'pos': 'DEM', 'cat': 'pn', 'gen': 'any', 'num': 'pl', 'pers': '3', 'case': 'd', 'vib': '', 'tam': '', 'chunkId': 'NP2', 'chunkType': 'child', 'stype': '', 'voicetype': ''}, {'pos': 'NN', 'cat': 'n', 'gen': 'm', 'num': 'pl', 'pers': '3', 'case': 'd', 'vib': '0', 'tam': '0', 'chunkId': 'NP2', 'chunkType': 'head', 'stype': '', 'voicetype': ''}, {'pos': 'NNP', 'cat': 'n', 'gen': 'm', 'num': 'sg', 'pers': '3', 'case': 'd', 'vib': '0', 'tam': '0', 'chunkId': 'NP3', 'chunkType': 'head', 'stype': '', 'voicetype': ''}, {'pos': 'JJ', 'cat': 'adj', 'gen': 'any', 'num': 'any', 'pers': '', 'case': 'o', 'vib': '', 'tam': '', 'chunkId': 'JJP', 'chunkType': 'head', 'stype': '', 'voicetype': ''}, {'pos': 'NNPC', 'cat': 'n', 'gen': 'm', 'num': 'sg', 'pers': '3', 'case': 'd', 'vib': '0', 'tam': '0', 'chunkId': 'NP4', 'chunkType': 'child', 'stype': '', 'voicetype': ''}, {'pos': 'NNPC', 'cat': 'n', 'gen': 'f', 'num': 'sg', 'pers': '3', 'case': 'd', 'vib': '0', 'tam': '0', 'chunkId': 'NP4', 'chunkType': 'child', 'stype': '', 'voicetype': ''}, {'pos': 'NNP', 'cat': 'n', 'gen': 'm', 'num': 'sg', 'pers': '3', 'case': 'o', 'vib': '0_के_बाहर', 'tam': '0', 'chunkId': 'NP4', 'chunkType': 'head', 'stype': '', 'voicetype': ''}, {'pos': 'PSP', 'cat': 'psp', 'gen': '', 'num': '', 'pers': '', 'case': '', 'vib': '', 'tam': '', 'chunkId': 'NP4', 'chunkType': 'child', 'stype': '', 'voicetype': ''}, {'pos': 'NST', 'cat': 'nst', 'gen': 'm', 'num': 'sg', 'pers': '3', 'case': 'd', 'vib': '', 'tam': '', 'chunkId': 'NP4', 'chunkType': 'child', 'stype': '', 'voicetype': ''}, {'pos': 'NNPC', 'cat': 'n', 'gen': 'm', 'num': 'sg', 'pers': '3', 'case': 'd', 'vib': '0', 'tam': '0', 'chunkId': 'NP5', 'chunkType': 'child', 'stype': '', 'voicetype': ''}, {'pos': 'NNP', 'cat': 'n', 'gen': 'f', 'num': 'sg', 'pers': '3', 'case': 'o', 'vib': '0_का', 'tam': '0', 'chunkId': 'NP5', 'chunkType': 'head', 'stype': '', 'voicetype': ''}, {'pos': 'PSP', 'cat': 'psp', 'gen': 'm', 'num': 'sg', 'pers': '', 'case': 'o', 'vib': '', 'tam': '', 'chunkId': 'NP5', 'chunkType': 'child', 'stype': '', 'voicetype': ''}, {'pos': 'JJC', 'cat': 'n', 'gen': 'f', 'num': 'sg', 'pers': '3', 'case': 'd', 'vib': '0', 'tam': '0', 'chunkId': 'NP6', 'chunkType': 'child', 'stype': '', 'voicetype': ''}, {'pos': 'JJ', 'cat': 'adj', 'gen': 'any', 'num': 'any', 'pers': '', 'case': 'o', 'vib': '', 'tam': '', 'chunkId': 'NP6', 'chunkType': 'child', 'stype': '', 'voicetype': ''}, {'pos': 'NN', 'cat': 'n', 'gen': 'm', 'num': 'sg', 'pers': '3', 'case': 'o', 'vib': '0_को', 'tam': '0', 'chunkId': 'NP6', 'chunkType': 'head', 'stype': '', 'voicetype': ''}, {'pos': 'PSP', 'cat': 'psp', 'gen': '', 'num': '', 'pers': '', 'case': '', 'vib': '', 'tam': '', 'chunkId': 'NP6', 'chunkType': 'child', 'stype': '', 'voicetype': ''}, {'pos': 'VM', 'cat': 'v', 'gen': 'any', 'num': 'sg', 'pers': 'any', 'case': 'o', 'vib': 'ना_का', 'tam': 'nA', 'chunkId': 'VGNN', 'chunkType': 'head', 'stype': '', 'voicetype': ''}, {'pos': 'PSP', 'cat': 'psp', 'gen': 'f', 'num': 'sg', 'pers': '', 'case': 'o', 'vib': '', 'tam': '', 'chunkId': 'VGNN', 'chunkType': 'child', 'stype': '', 'voicetype': ''}, {'pos': 'NN', 'cat': 'n', 'gen': 'f', 'num': 'sg', 'pers': '3', 'case': 'o', 'vib': '0_में', 'tam': '0', 'chunkId': 'NP7', 'chunkType': 'head', 'stype': '', 'voicetype': ''}, {'pos': 'PSP', 'cat': 'psp', 'gen': '', 'num': '', 'pers': '', 'case': '', 'vib': '', 'tam': '', 'chunkId': 'NP7', 'chunkType': 'child', 'stype': '', 'voicetype': ''}, {'pos': 'VM', 'cat': 'v', 'gen': 'm', 'num': 'pl', 'pers': 'any', 'case': '', 'vib': 'था', 'tam': 'WA', 'chunkId': 'VGF2', 'chunkType': 'head', 'stype': 'declarative', 'voicetype': 'active'}, {'pos': 'SYM', 'cat': 'punc', 'gen': '', 'num': '', 'pers': '', 'case': '', 'vib': '', 'tam': '', 'chunkId': 'BLK', 'chunkType': 'head', 'stype': '', 'voicetype': ''}]\n"
     ]
    }
   ],
   "source": [
    "# with open('Data/Testing_features.pkl', 'rb') as pickled:\n",
    "#     newlist = pickle.load(pickled)\n",
    "# print(len(newlist))\n",
    "# print(newlist[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
